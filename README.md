# ğŸš€ Real-Time Data Pipeline with API, AWS Lambda, and Snowflake

This project demonstrates how to design and implement a **real-time data pipeline** using:

- **API integration** for data ingestion  
- **AWS Lambda** for event-driven processing  
- **Snowflake** for scalable data storage & transformation  
- **Power BI** for dashboarding and analytics  

---

## ğŸ“Œ Architecture

![Architecture](folder/Screenshot%202025-09-30%20125932.png)

The pipeline flow:

1. **API â†’ Lambda** â†’ Fetch & validate data.  
2. **Lambda â†’ S3 Raw Zone** â†’ Store data in JSONL/CSV.  
3. **Snowpipe â†’ Snowflake RAW schema** â†’ Auto-ingest from S3.  
4. **Streams/Tasks â†’ Snowflake CORE schema** â†’ ELT transformations.  
5. **Power BI** â†’ Visualization and analytics.  

---

## ğŸ“‚ Repository Structure

---

## âš™ï¸ Technologies Used

- **AWS Lambda** â†’ serverless ingestion & automation  
- **Amazon S3** â†’ raw storage layer  
- **Snowflake** â†’ cloud data warehouse for ELT & analytics  
- **Power BI** â†’ dashboard & visualization  
- **Python** â†’ API fetch, ETL scripts  
- **GitHub Desktop** â†’ version control  

---

## ğŸš€ How It Works

1. **API Data Fetch**  
   - `Fetch_data_from_API.ipynb` tests API endpoints.  
   - Lambda (`lambda_function.py`) pulls live data from the API.  

2. **Lambda Processing**  
   - Validates & stores JSONL/CSV in S3 (`raw/` prefix).  

3. **Snowflake Ingestion**  
   - `RAW` schema ingests via **Snowpipe**.  
   - Data transformations handled via **Streams** & **Tasks** into `CORE`.  

4. **Visualization**  
   - Power BI connects to `CORE` schema in Snowflake.  
   - Dashboards update automatically via DirectQuery or scheduled refresh.  

---

## ğŸ“‘ Documentation

- [Solution Methodology (PDF)](folder/Solution%20Methodology%202.pdf)

---

## âš ï¸ Dataset Notice

The full dataset (`total_data.csv`) is **not included** in this repository due to GitHubâ€™s 100 MB file size limit.  
ğŸ‘‰ Use your own dataset or download from the original source/S3 bucket.  
ğŸ‘‰ A small sample (`sample_data/sample_1k.jsonl`) is included for testing.  

---

## ğŸ› ï¸ Deployment Steps

### 1. Snowflake Setup
- Run `code/snowflake/00_roles_warehouses.sql` â†’ create roles/warehouse.  
- Run `10_raw_objects.sql` â†’ DB, schemas, stage.  
- Run `40_snowpipe.sql` â†’ create pipe, connect to S3 notifications.  

### 2. AWS Setup
- Create S3 bucket (e.g., `rt-pipeline-raw`).  
- Deploy Lambda with environment variables:  

