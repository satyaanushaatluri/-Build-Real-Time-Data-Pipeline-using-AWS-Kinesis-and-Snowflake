# ğŸš€ Build Real-Time Data Pipeline with API, Lambda, and Snowflake

This project demonstrates how to design and implement a **real-time data pipeline** using:
- **API integration** for data ingestion
- **AWS Lambda** for event-driven processing
- **Snowflake** for scalable data storage & transformation
- **Power BI** for dashboarding and analytics

---

## ğŸ“Œ Architecture
![Architecture](docs/architecture.png)

---

## ğŸ“‚ Repository Structure

---

## âš™ï¸ Technologies Used
- **AWS Lambda** â†’ serverless function for pipeline automation  
- **Snowflake** â†’ cloud data warehouse for storage & queries  
- **Power BI** â†’ dashboard and visualization  
- **Python** â†’ API fetch, ETL scripts, notebooks  
- **GitHub Desktop** â†’ version control  

---

## ğŸš€ How It Works
1. **API Data Fetch** â†’ scripts in `code/Fetch_data_from_API.ipynb` fetch data from an external API.  
2. **Lambda Processing** â†’ `code/lambda_function.py` handles real-time data ingestion.  
3. **Snowflake Storage** â†’ data is loaded into Snowflake tables for analytics.  
4. **Visualization** â†’ Power BI dashboard (`code/snowflake-dashboard.pbix`) shows insights.  

---

## ğŸ“‘ Documentation
- [Solution Methodology (PDF)](docs/Solution-Methodology.pdf)

---

## âš ï¸ Dataset Notice
The full dataset (`total_data.csv`) is **not included** in this repository due to GitHubâ€™s 100 MB file size limit.  
ğŸ‘‰ Use your own dataset or download from the original source/S3 bucket.  

---

## âœ¨ Author
**Satya Anusha Atluri**
